{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Project-1:-Airline-Flight\" data-toc-modified-id=\"Project-1:-Airline-Flight-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Project 1: Airline Flight</a></span></li><li><span><a href=\"#Data-cleaning:\" data-toc-modified-id=\"Data-cleaning:-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data cleaning:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Indexing:-Categorical-data-to-numerical\" data-toc-modified-id=\"Indexing:-Categorical-data-to-numerical-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Indexing: Categorical data to numerical</a></span></li><li><span><a href=\"#Assembling-columns\" data-toc-modified-id=\"Assembling-columns-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Assembling columns</a></span></li></ul></li><li><span><a href=\"#Classification:-Decision-Tree\" data-toc-modified-id=\"Classification:-Decision-Tree-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Classification: Decision Tree</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression-model\" data-toc-modified-id=\"Logistic-Regression-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Logistic Regression model</a></span></li><li><span><a href=\"#Evaluate-the-Logistic-Regression-model\" data-toc-modified-id=\"Evaluate-the-Logistic-Regression-model-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Evaluate the Logistic Regression model</a></span></li></ul></li><li><span><a href=\"#Regression:-Predicting-flight-delay-by-the-distance-and-airport:\" data-toc-modified-id=\"Regression:-Predicting-flight-delay-by-the-distance-and-airport:-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Regression: Predicting flight delay by the distance and airport:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Encoding-flight-origin\" data-toc-modified-id=\"Encoding-flight-origin-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Encoding flight origin</a></span></li></ul></li><li><span><a href=\"#Feature-engineering:--Adding-departure-time\" data-toc-modified-id=\"Feature-engineering:--Adding-departure-time-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Feature engineering:  Adding departure time</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bucketing\" data-toc-modified-id=\"Bucketing-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Bucketing</a></span></li><li><span><a href=\"#Redo-Regression\" data-toc-modified-id=\"Redo-Regression-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Redo Regression</a></span></li></ul></li><li><span><a href=\"#Regularization:-Feature-selection\" data-toc-modified-id=\"Regularization:-Feature-selection-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Regularization: Feature selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Implementration-to-our-data\" data-toc-modified-id=\"Implementration-to-our-data-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Implementration to our data</a></span></li></ul></li><li><span><a href=\"#SMS-spam-data\" data-toc-modified-id=\"SMS-spam-data-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>SMS spam data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preprocessing:\" data-toc-modified-id=\"Data-Preprocessing:-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Data Preprocessing:</a></span></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Logistic Regression</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer).It is much faster that hadoop. Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\n",
    "\n",
    "The SparkSession class has a builder attribute, which is an instance of the Builder class. The Builder class exposes three important methods that let you:\n",
    "\n",
    "- specify the location of the master node;\n",
    "- name the application (optional); and\n",
    "- retrieve an existing SparkSession or, if there is none, create a new one.\n",
    "The SparkSession class has a version attribute which gives the version of Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('C:/Users/Hamid/Anaconda3/lib/site-packages/pyspark')\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.4\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession object\n",
    "spark = SparkSession.builder.master('local[*]').appName('test').getOrCreate()\n",
    "\n",
    "# What version of Spark?\n",
    "print(spark.version)\n",
    "\n",
    "# Terminate the cluster\n",
    "#spark.stop()\n",
    "\n",
    "#SparkSession._instantiatedContext = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Project 1: Airline Flight\n",
    "We will develope a model which predice whether or not a given flight will be delayed.  \n",
    "\n",
    "read.csv methode read all the data as a string as default. To make it correct we need to use  inferSchema=True. However, It need another checking of the data which can take so much time if data is big. \n",
    "\n",
    "#  Data cleaning:\n",
    "\n",
    "- removing an uninformative column \n",
    "- removing rows which do not have information about whether or not a flight was delayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 50000 records.\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mon', 'int'),\n",
       " ('dom', 'int'),\n",
       " ('dow', 'int'),\n",
       " ('carrier', 'string'),\n",
       " ('flight', 'int'),\n",
       " ('org', 'string'),\n",
       " ('mile', 'int'),\n",
       " ('depart', 'double'),\n",
       " ('duration', 'int'),\n",
       " ('delay', 'int')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from CSV file, inferSchema - deduce column data types from data.\n",
    "flights = spark.read.csv('flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "flights.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intial 50000 data contain 47022 not null records.\n"
     ]
    }
   ],
   "source": [
    "# Remove the 'flight' column\n",
    "flights_drop_column = flights.drop('flight')\n",
    "\n",
    "# Number of records with missing 'delay' values. delay IS NULL is sql syntax\n",
    "flights_drop_column.filter('delay IS NULL').count()\n",
    "\n",
    "# Remove records with missing 'delay' values\n",
    "flights = flights_drop_column.filter('delay IS NOT NULL')\n",
    "\n",
    "# Remove records with missing values in any column and get the number of remaining rows\n",
    "flights_none_missing = flights.dropna()\n",
    "print(\"Intial 50000 data contain %d not null records.\" % flights_none_missing.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Federal Aviation Administration (FAA) considers a flight to be \"delayed\" when it arrives 15 minutes or more after its scheduled time.\n",
    "\n",
    "The next step of preparing the flight data has two parts:\n",
    "- the units of distance, replacing the mile column with a km column; and\n",
    "- create a Boolean column indicating whether or not a flight was delayed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Spark withColumn() function is used to rename, change the value, convert the datatype of an existing DataFrame column and also can be used to create a new column, on this post, I will walk you through commonly used DataFrame column operations with Scala and Pyspark examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the required function\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert 'mile' to 'km' and drop 'mile' column\n",
    "flights_km = flights.withColumn('km', round(flights.mile * 1.60934, 0)) \\\n",
    "                    .drop('mile')\n",
    "\n",
    "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
    "# with cast we change the datatype to integer\n",
    "flights = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
    "\n",
    "# Check first five records\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing: Categorical data to numerical\n",
    "In the flights data there are two columns, carrier and org, which hold categorical data. We need to transform those columns into indexed numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create an indexer\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "\n",
    "# Indexer identifies categories in the data\n",
    "indexer_model = indexer.fit(flights)\n",
    "\n",
    "# Indexer creates a new column with numeric index values\n",
    "flights_indexed = indexer_model.transform(flights)\n",
    "\n",
    "# Repeat the process for the other categorical feature\n",
    "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)\n",
    "\n",
    "flights_indexed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling columns\n",
    "The final stage of data preparation is to consolidate all of the predictor columns into a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+\n",
      "|features                                 |delay|\n",
      "+-----------------------------------------+-----+\n",
      "|[0.0,22.0,2.0,0.0,0.0,509.0,16.33,82.0]  |30   |\n",
      "|[2.0,20.0,4.0,0.0,1.0,542.0,6.17,82.0]   |-8   |\n",
      "|[9.0,13.0,1.0,1.0,0.0,1989.0,10.33,195.0]|-5   |\n",
      "|[5.0,2.0,1.0,0.0,1.0,885.0,7.98,102.0]   |2    |\n",
      "|[7.0,2.0,6.0,1.0,0.0,1180.0,10.83,135.0] |54   |\n",
      "+-----------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow', 'carrier_idx', 'org_idx', 'km', 'depart', 'duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights_indexed)\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Decision Tree\n",
    "1. Splilt data into train and test\n",
    "2. improt object\n",
    "3. Prediction and test on the test set. \n",
    "4. Creat confusion matrix\n",
    "\n",
    "\n",
    "\n",
    "A confusion matrix gives a useful breakdown of predictions versus known values. It has four cells which represent the counts of:\n",
    "\n",
    "- True Negatives (TN) — model predicts negative outcome & known outcome is negative\n",
    "- True Positives (TP) — model predicts positive outcome & known outcome is positive\n",
    "- False Negatives (FN) — model predicts negative outcome but known outcome is positive\n",
    "- False Positives (FP) — model predicts positive outcome but known outcome is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train to test ratio= 0.7980732423121092\n",
      "Probability of predictions:\n",
      "+-----+----------+---------------------------------------+\n",
      "|label|prediction|probability                            |\n",
      "+-----+----------+---------------------------------------+\n",
      "|1    |1.0       |[0.4939987365761213,0.5060012634238787]|\n",
      "|1    |1.0       |[0.3550259700580507,0.6449740299419493]|\n",
      "|1    |1.0       |[0.3550259700580507,0.6449740299419493]|\n",
      "|1    |1.0       |[0.3550259700580507,0.6449740299419493]|\n",
      "|1    |1.0       |[0.3550259700580507,0.6449740299419493]|\n",
      "+-----+----------+---------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Confusion matrix:\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1205|\n",
      "|    0|       0.0| 2400|\n",
      "|    1|       1.0| 3619|\n",
      "|    0|       1.0| 2271|\n",
      "+-----+----------+-----+\n",
      "\n",
      "accuracy= 0.6339125855713533\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = flights_train.count() / flights.count()\n",
    "print('train to test ratio=',training_ratio)\n",
    "\n",
    "\n",
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(flights_train)\n",
    "\n",
    "print(\"Probability of predictions:\")\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(flights_test)\n",
    "\n",
    "\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)\n",
    "\n",
    "# Create a confusion matrix\n",
    "print(\"Confusion matrix:\")\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "print('accuracy=', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model\n",
    "The objective is to predict whether a flight is likely to be delayed by at least 15 minutes (label 1) or not (label 0).\n",
    "\n",
    "## Evaluate the Logistic Regression model\n",
    "\n",
    "Accuracy is generally not a very reliable metric because it can be biased by the most common target class\n",
    "Precision is the proportion of positive predictions which are correct. For all flights which are predicted to be delayed, what proportion is actually delayed?\n",
    "\n",
    "Recall is the proportion of positives outcomes which are correctly predicted. For all delayed flights, what proportion is correctly predicted by the model?\n",
    "\n",
    "The precision and recall are generally formulated in terms of the positive target class. But it's also possible to calculate weighted versions of these metrics which look at both target classes.\n",
    "\n",
    "The components of the confusion matrix are available as TN, TP, FN and FP, as well as the object prediction\n",
    "\n",
    "\n",
    "ROC and AUC : The ROC curve plots the true positive rate versus the false positve for different threshold values.\n",
    "AUS summersizes the ROC in single number which is the area under ROC curve.\n",
    "\n",
    "A prefect model which act prefectly regardless to treshhold value will have AUC of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1652|\n",
      "|    0|       0.0| 2645|\n",
      "|    1|       1.0| 3172|\n",
      "|    0|       1.0| 2026|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the logistic regression class\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a classifier object and train on training data\n",
    "logistic = LogisticRegression().fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and show confusion matrix\n",
    "prediction = logistic.transform(flights_test)\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.61\n",
      "recall    = 0.75\n",
      "auc = 0.65\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
    "\n",
    "# Find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
    "print('auc = {:.2f}'.format(auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression: Predicting flight delay by the distance and airport:\n",
    "We want to see the distance and  type of airport can effect the delay or not. Bigger airport might take more time for checkin.\n",
    "\n",
    "So in assembler we just use column 'km' and origin airport (which is encoded)  as features and duration is the prediction. \n",
    "\n",
    "\n",
    "Interpreting the coefficients\n",
    "The linear regression model for flight duration as a function of distance takes the form\n",
    "\n",
    "duration=α+β×distance+β1×airport\n",
    "where\n",
    "\n",
    "α — intercept (component of duration which does not depend on distance) and\n",
    "β — coefficient (rate at which duration increases as a function of distance; also called the slope)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding flight origin\n",
    "The org column in the flights data is a categorical variable giving the airport from which a flight departs.\n",
    "\n",
    "ORD — O'Hare International Airport (Chicago)\n",
    "\n",
    "SFO — San Francisco International Airport\n",
    "\n",
    "JFK — John F Kennedy International Airport (New York)\n",
    "\n",
    "LGA — La Guardia Airport (New York)\n",
    "\n",
    "SMF — Sacramento\n",
    "\n",
    "SJC — San Jose\n",
    "\n",
    "TUS — Tucson International Airport\n",
    "\n",
    "OGG — Kahului (Hawaii)\n",
    "\n",
    "\n",
    "For this part we used the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+\n",
      "|org|org_idx|    org_dummy|\n",
      "+---+-------+-------------+\n",
      "|ORD|    0.0|(7,[0],[1.0])|\n",
      "|SFO|    1.0|(7,[1],[1.0])|\n",
      "|JFK|    2.0|(7,[2],[1.0])|\n",
      "|LGA|    3.0|(7,[3],[1.0])|\n",
      "|SMF|    4.0|(7,[4],[1.0])|\n",
      "|SJC|    5.0|(7,[5],[1.0])|\n",
      "|TUS|    6.0|(7,[6],[1.0])|\n",
      "|OGG|    7.0|    (7,[],[])|\n",
      "+---+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the one hot encoder class\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "# Create an instance of the one hot encoder\n",
    "onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# Apply the one hot encoder to the flights data\n",
    "onehot = onehot.fit(flights_indexed)\n",
    "flights_onehot = onehot.transform(flights_indexed)\n",
    "\n",
    "# Check the results\n",
    "flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy'], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights_onehot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+\n",
      "|features              |delay|\n",
      "+----------------------+-----+\n",
      "|(8,[0,1],[509.0,1.0]) |30   |\n",
      "|(8,[0,2],[542.0,1.0]) |-8   |\n",
      "|(8,[0,1],[1989.0,1.0])|-5   |\n",
      "|(8,[0,2],[885.0,1.0]) |2    |\n",
      "|(8,[0,1],[1180.0,1.0])|54   |\n",
      "+----------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+------------------+\n",
      "|duration|prediction        |\n",
      "+--------+------------------+\n",
      "|240     |228.3383341007048 |\n",
      "|160     |150.56907837678995|\n",
      "|130     |132.0788580796877 |\n",
      "|275     |265.0747799757123 |\n",
      "|85      |92.64790636177084 |\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      " Average speed is 807 km per houre\n",
      "Intercept 15.536278501461982\n",
      "Average minutes on ground at JFK= 68.2929992642743\n",
      "Average minutes on ground at LGA= 62.37189890382314\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled.select('features', 'delay').show(5, truncate=False)\n",
    "\n",
    "\n",
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(flights_test)\n",
    "predictions.select('duration', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
    "\n",
    "# Average speed in km per hour\n",
    "avg_speed_hour = 60 / regression.coefficients[0]\n",
    "print(\" Average speed is %d km per houre\" % avg_speed_hour)\n",
    "\n",
    "\n",
    "# Intercept (average minutes on ground)\n",
    "inter = regression.intercept\n",
    "print('Intercept',inter)\n",
    "\n",
    "# Average minutes on ground at JFK\n",
    "avg_ground_jfk = inter + regression.coefficients[3]\n",
    "print('Average minutes on ground at JFK=',avg_ground_jfk)\n",
    "\n",
    "# Average minutes on ground at LGA\n",
    "avg_ground_lga = inter + regression.coefficients[4]\n",
    "print('Average minutes on ground at LGA=',avg_ground_lga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering:  Adding departure time\n",
    "## Bucketing\n",
    "Here we work on manipulating features. we assign a lable for each range of an specific feature. The resulting categorical variable is often more powerfull. \n",
    "Then we make them one-hot encoded. \n",
    "The main  goal is to produce new features which has more effect on the target.\n",
    "In our dataset we convert the flight departure times from numeric values between 0 (corresponding to 00:00) and 24 (corresponding to 24:00) to binned values. You'll then take those binned values and one-hot encode them.\n",
    "\n",
    "\n",
    "The data are in flights. The km, org_dummy and depart_dummy columns have been assembled into features, where km is index 0, org_dummy runs from index 1 to 7 and depart_dummy from index 8 to 14.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|depart|depart_bucket|\n",
      "+------+-------------+\n",
      "| 16.33|          5.0|\n",
      "|  6.17|          2.0|\n",
      "| 10.33|          3.0|\n",
      "|  7.98|          2.0|\n",
      "| 10.83|          3.0|\n",
      "+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-------------+-------------+\n",
      "|depart|depart_bucket| depart_dummy|\n",
      "+------+-------------+-------------+\n",
      "| 16.33|          5.0|(7,[5],[1.0])|\n",
      "|  6.17|          2.0|(7,[2],[1.0])|\n",
      "| 10.33|          3.0|(7,[3],[1.0])|\n",
      "|  7.98|          2.0|(7,[2],[1.0])|\n",
      "| 10.83|          3.0|(7,[3],[1.0])|\n",
      "+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer, OneHotEncoderEstimator\n",
    "\n",
    "# Create buckets at 3 hour intervals through the day\n",
    "buckets = Bucketizer(splits=[0, 3, 6, 9, 12, 15, 18, 21, 24], inputCol='depart', outputCol='depart_bucket')\n",
    "\n",
    "# Bucket the departure times\n",
    "bucketed = buckets.transform(flights_onehot)\n",
    "bucketed.select('depart', 'depart_bucket').show(5)\n",
    "\n",
    "# Create a one-hot encoder for depart_bucket\n",
    "onehot = OneHotEncoderEstimator(inputCols=['depart_bucket'], outputCols=['depart_dummy'])\n",
    "\n",
    "# One-hot encode the bucketed departure times\n",
    "flights_onehot_bucket = onehot.fit(bucketed).transform(bucketed)\n",
    "flights_onehot_bucket.select('depart', 'depart_bucket', 'depart_dummy').show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "     'km', 'org_dummy', 'depart_dummy'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled_kod = assembler.transform(flights_onehot_bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+\n",
      "|features                      |delay|\n",
      "+------------------------------+-----+\n",
      "|(15,[0,1,13],[509.0,1.0,1.0]) |30   |\n",
      "|(15,[0,2,10],[542.0,1.0,1.0]) |-8   |\n",
      "|(15,[0,1,11],[1989.0,1.0,1.0])|-5   |\n",
      "|(15,[0,2,10],[885.0,1.0,1.0]) |2    |\n",
      "|(15,[0,1,11],[1180.0,1.0,1.0])|54   |\n",
      "+------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+-------------+--------------------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|    org_dummy|depart_bucket| depart_dummy|            features|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+-------------+--------------------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|(7,[0],[1.0])|          5.0|(7,[5],[1.0])|(15,[0,1,13],[509...|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|(7,[1],[1.0])|          2.0|(7,[2],[1.0])|(15,[0,2,10],[542...|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|(7,[0],[1.0])|          3.0|(7,[3],[1.0])|(15,[0,1,11],[198...|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|(7,[1],[1.0])|          2.0|(7,[2],[1.0])|(15,[0,2,10],[885...|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|(7,[0],[1.0])|          3.0|(7,[3],[1.0])|(15,[0,1,11],[118...|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+-------------+-------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+------------------+\n",
      "|duration|prediction        |\n",
      "+--------+------------------+\n",
      "|240     |226.15246196971202|\n",
      "|160     |148.66611481191933|\n",
      "|130     |134.3697871397007 |\n",
      "|275     |267.4716225570108 |\n",
      "|85      |85.95060436743158 |\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "10.029213579060977\n",
      "-3.404434791432136\n",
      "48.494562005382264\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled_kod.select('features', 'delay').show(5, truncate=False)\n",
    "flights_assembled_kod.show(5)\n",
    "\n",
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights_assembled_kod.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(flights_test)\n",
    "predictions.select('duration', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
    "\n",
    "# Average minutes on ground at OGG for flights departing between 21:00 and 24:00\n",
    "avg_eve_ogg = regression.intercept\n",
    "print(avg_eve_ogg)\n",
    "\n",
    "# Average minutes on ground at OGG for flights departing between 00:00 and 03:00\n",
    "avg_night_ogg = regression.intercept + regression.coefficients[8]\n",
    "print(avg_night_ogg)\n",
    "\n",
    "# Average minutes on ground at JFK for flights departing between 00:00 and 03:00\n",
    "avg_night_jfk = regression.intercept + regression.coefficients[8] + regression.coefficients[3]\n",
    "print(avg_night_jfk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization: Feature selection\n",
    "is usefull for feature selection when we have many column. An extra regularization term is added to the loss function.\n",
    "The regularization term can be either\n",
    "- Lasso — absolute value ofthe coefcients\n",
    "- Ridge — square ofthe coefcients\n",
    "\n",
    "It's also possible to have a blend of Lasso and Ridge regression.\n",
    "\n",
    "Strength of regularization determined by parameter λ:\n",
    "\n",
    "λ = 0 — no regularization (standard regression) \n",
    "\n",
    "λ = ∞ — complete regularization (all coefcients zero)\n",
    "\n",
    "In lasso all values can be close to zero but it Ridge they could be zero.\n",
    "Elastic net parameter is a hyperpatameter which should be zero.\n",
    "\n",
    "\n",
    "## Implementration to our data\n",
    "More features will always make the model more complicated and difficult to interpret.\n",
    "\n",
    "These are the features you'll include in the next model:\n",
    "\n",
    "- km\n",
    "- org (origin airport, one-hot encoded, 8 levels)\n",
    "- depart (departure time, binned in 3 hour intervals, one-hot encoded, 8 levels)\n",
    "- dow (departure day of week, one-hot encoded, 7 levels) and\n",
    "- mon (departure month, one-hot encoded, 12 levels).\n",
    "- These have been assembled into the features column, which is a sparse representation of 32 columns (remember one-hot encoding produces a number of columns which is one fewer than the number of levels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-------------+-------------+--------------+\n",
      "|    km| depart_dummy|    org_dummy|    dow_dummy|     mon_dummy|\n",
      "+------+-------------+-------------+-------------+--------------+\n",
      "|3954.0|(7,[5],[1.0])|(7,[1],[1.0])|    (6,[],[])|(11,[0],[1.0])|\n",
      "|1754.0|(7,[1],[1.0])|(7,[2],[1.0])|(6,[2],[1.0])|(11,[9],[1.0])|\n",
      "| 301.0|(7,[2],[1.0])|(7,[2],[1.0])|(6,[5],[1.0])|(11,[6],[1.0])|\n",
      "| 542.0|(7,[2],[1.0])|(7,[1],[1.0])|(6,[2],[1.0])|(11,[9],[1.0])|\n",
      "| 425.0|    (7,[],[])|(7,[0],[1.0])|(6,[5],[1.0])|(11,[4],[1.0])|\n",
      "+------+-------------+-------------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the one hot encoder class\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "# Create an instance of the one hot encoder\n",
    "onehot = OneHotEncoderEstimator(inputCols=['dow'], outputCols=['dow_dummy'])\n",
    "\n",
    "# Apply the one hot encoder to the flights data\n",
    "onehot = onehot.fit(flights_assembled_kod)\n",
    "flights_onehot_a = onehot.transform(flights_assembled_kod)\n",
    "\n",
    "# Create an instance of the one hot encoder\n",
    "onehot = OneHotEncoderEstimator(inputCols=['mon'], outputCols=['mon_dummy'])\n",
    "\n",
    "# Apply the one hot encoder to the flights data\n",
    "onehot = onehot.fit(flights_onehot_a)\n",
    "flights_onehot_all = onehot.transform(flights_onehot_a)\n",
    "flights_onehot_all= flights_onehot_all.drop('features')\n",
    "# Check the results\n",
    "flights_onehot_all.select( 'km','depart_dummy', 'org_dummy', 'dow_dummy', 'mon_dummy').distinct().show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "     'km', 'org_dummy', 'depart_dummy', 'dow_dummy', 'mon_dummy'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled_kod_5f = assembler.transform(flights_onehot_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is 11.65852539759888\n",
      "[0.07347781392862278,5.705478011506563,0.0,28.958072294272437,21.75674278399281,-2.342239176353107,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0653074231639355,1.2051514011784286,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "Number of coefficients equal to 0: 25\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights_assembled_kod_5f.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# Fit Lasso model (α = 1) to training data\n",
    "regression = LinearRegression(labelCol='duration', regParam=1, elasticNetParam=1).fit(flights_train)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol='duration').evaluate(regression.transform(flights_test))\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)\n",
    "\n",
    "# Number of zero coefficients\n",
    "zero_coeff = sum([beta == 0 for beta in regression.coefficients])\n",
    "print(\"Number of coefficients equal to 0:\", zero_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  SMS spam data\n",
    "The file sms.csv contains a selection of SMS messages which have been classified as either 'spam' or 'ham'. These data have been adapted from the UCI Machine Learning Repository. There are a total of 5574 SMS, of which 747 have been labelled as spam.\n",
    "\n",
    "Notes on CSV format:\n",
    "\n",
    "- no header record and\n",
    "- fields are separated by a semicolon (this is not the default separator).\n",
    "\n",
    "Data dictionary:\n",
    "- id — record identifier\n",
    "- text — content of SMS message\n",
    "- label — spam or ham (integer; 0 = ham and 1 = spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "| id|                text|label|\n",
      "+---+--------------------+-----+\n",
      "|  1|Sorry, I'll call ...|    0|\n",
      "|  2|Dont worry. I gue...|    0|\n",
      "|  3|Call FREEPHONE 08...|    1|\n",
      "|  4|Win a 1000 cash p...|    1|\n",
      "|  5|Go until jurong p...|    0|\n",
      "+---+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "sms = spark.read.csv(\"sms.csv\", sep=';', header=False, schema=schema)\n",
    "\n",
    "# View the first five records\n",
    "sms.show(5)\n",
    "\n",
    "\n",
    "# Print schema of DataFrame\n",
    "sms.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preprocessing:\n",
    "- Removing puctuation and numbers: by using of regular exression or REGEX\n",
    "- Tokenizing\n",
    "- StopWordsRemover\n",
    "- Feature hashing: After these two steps the document might contain a larg variety of words. It is better to convert it to numbers.  numFeatures=1024: The largest number which can be produced by hashing trick. It  should be big to capture diversity of the words\n",
    "- Convert to TF-IDF representation. The TF-IDF matrix reflects how important a word is to each document. It takes into account both the frequency of the word within each document but also the frequency of the word across all of the documents in the collection.\n",
    "\n",
    "- Dealing with commond words appear frequently across many documents.  \n",
    "If a word appears in many documents then it is less usefull for buidling a classifier. we weight the number of counts for a word in a particular document against how frequenty that word occurs across all documents. To do this we reduce the effective count for more common  words giving what is known as the 'Inverse document frequency.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|id |text                              |label|words                                     |\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "|1  |Sorry I'll call later in meeting  |0    |[sorry, i'll, call, later, in, meeting]   |\n",
      "|2  |Dont worry I guess he's busy      |0    |[dont, worry, i, guess, he's, busy]       |\n",
      "|3  |Call FREEPHONE now                |1    |[call, freephone, now]                    |\n",
      "|4  |Win a cash prize or a prize worth |1    |[win, a, cash, prize, or, a, prize, worth]|\n",
      "+---+----------------------------------+-----+------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary functions\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# Remove punctuation (REGEX provided) and numbers\n",
    "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
    "\n",
    "# Merge multiple spaces\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
    "\n",
    "# Split the text into words\n",
    "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
    "\n",
    "wrangled.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|terms                           |features                                                                                            |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|[sorry, call, later, meeting]   |(1024,[138,344,378,1006],[2.2391682769656747,2.892706319430574,3.684405173719015,4.244020961654438])|\n",
      "|[dont, worry, guess, busy]      |(1024,[53,233,329,858],[4.618714411095849,3.557143394108088,4.618714411095849,4.937168142214383])   |\n",
      "|[call, freephone]               |(1024,[138,396],[2.2391682769656747,3.3843005812686773])                                            |\n",
      "|[win, cash, prize, prize, worth]|(1024,[31,69,387,428],[3.7897656893768414,7.284881949239966,4.4671645129686475,3.898659777615979])  |\n",
      "+--------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Remove stop words.\n",
    "wrangled = StopWordsRemover(inputCol='words', outputCol='terms')\\\n",
    "      .transform(wrangled)\n",
    "\n",
    "# Apply the hashing trick\n",
    "wrangled = HashingTF(inputCol='terms', outputCol='hash', numFeatures=1024)\\\n",
    "      .transform(wrangled)\n",
    "\n",
    "# Convert hashed symbols to TF-IDF\n",
    "tf_idf = IDF(inputCol='hash', outputCol='features')\\\n",
    "      .fit(wrangled).transform(wrangled)\n",
    "      \n",
    "tf_idf.select('terms', 'features').show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|  119|\n",
      "|    0|       0.0| 2431|\n",
      "|    1|       1.0|  272|\n",
      "|    0|       1.0|   11|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "sms_train, sms_test = tf_idf.randomSplit([0.5, 0.5], seed=13)\n",
    "\n",
    "# Fit a Logistic Regression model to the training data\n",
    "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "prediction = logistic.transform(sms_test)\n",
    "\n",
    "# Create a confusion matrix, comparing predictions to known labels\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
